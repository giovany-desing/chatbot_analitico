"""
Script de monitoreo semanal para exportaci√≥n autom√°tica de datos de reentrenamiento.
Verifica si han pasado 7 d√≠as desde la √∫ltima exportaci√≥n y ejecuta exportaci√≥n si es necesario.
"""

import sys
import os
from pathlib import Path

# Agregar el directorio ra√≠z del proyecto al PYTHONPATH
project_root = Path(__file__).parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

import json
import logging
import shutil
import argparse
from datetime import datetime, timedelta
from typing import Optional, Dict, List, Tuple
from sqlalchemy import text
import time

from app.db.connections import get_postgres
from app.metrics.performance_tracker import track_hybrid_execution
from scripts.auto_export_training_data import export_training_data

# Configuraci√≥n por defecto (puede ser sobrescrita por .env)
RETRAINING_EXPORT_ENABLED = os.getenv("RETRAINING_EXPORT_ENABLED", "true").lower() == "true"
RETRAINING_EXPORT_INTERVAL_DAYS = int(os.getenv("RETRAINING_EXPORT_INTERVAL_DAYS", "7"))
RETRAINING_EXPORT_MIN_SAMPLES = int(os.getenv("RETRAINING_EXPORT_MIN_SAMPLES", "50"))
RETRAINING_CLEANUP_DAYS = int(os.getenv("RETRAINING_CLEANUP_DAYS", "30"))
RETRAINING_KEEP_LAST_N = int(os.getenv("RETRAINING_KEEP_LAST_N", "5"))

# Configurar logging estructurado
LOG_DIR = Path("logs")
LOG_DIR.mkdir(exist_ok=True)

LOG_FILE = LOG_DIR / "retraining_export.log"

# Configurar logger
logger = logging.getLogger("retraining_export")
logger.setLevel(logging.INFO)

# Handler para archivo
file_handler = logging.FileHandler(LOG_FILE, encoding='utf-8')
file_handler.setLevel(logging.INFO)

# Handler para consola
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)

# Formato estructurado
formatter = logging.Formatter(
    '[RETRAINING-EXPORT] %(asctime)s %(levelname)s %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)

file_handler.setFormatter(formatter)
console_handler.setFormatter(formatter)

logger.addHandler(file_handler)
logger.addHandler(console_handler)


# ============ Gesti√≥n de Tabla de Metadata ============

def create_export_metadata_table() -> None:
    """Crea la tabla export_metadata si no existe"""
    try:
        postgres = get_postgres()
        session = postgres.get_session()
        
        create_table_sql = """
        CREATE TABLE IF NOT EXISTS export_metadata (
            id SERIAL PRIMARY KEY,
            export_type VARCHAR(50) NOT NULL,
            last_export_date TIMESTAMP NOT NULL,
            records_exported INT,
            output_file TEXT,
            metadata JSONB,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        );
        
        CREATE INDEX IF NOT EXISTS idx_export_type ON export_metadata(export_type);
        CREATE INDEX IF NOT EXISTS idx_last_export_date ON export_metadata(last_export_date DESC);
        """
        
        session.execute(text(create_table_sql))
        session.commit()
        session.close()
        
        logger.info("‚úÖ Tabla export_metadata creada/verificada")
        
    except Exception as e:
        logger.error(f"‚ùå Error creando tabla export_metadata: {e}", exc_info=True)
        raise


def get_last_export_date(export_type: str = "retraining") -> Optional[datetime]:
    """
    Consulta export_metadata para obtener √∫ltima exportaci√≥n.
    
    Args:
        export_type: Tipo de exportaci√≥n (default: "retraining")
    
    Returns:
        datetime de √∫ltima exportaci√≥n o None si nunca se ha exportado
    """
    try:
        postgres = get_postgres()
        session = postgres.get_session()
        
        sql = """
        SELECT last_export_date
        FROM export_metadata
        WHERE export_type = :export_type
        ORDER BY last_export_date DESC
        LIMIT 1
        """
        
        result = session.execute(
            text(sql),
            {'export_type': export_type}
        ).fetchone()
        
        session.close()
        
        if result and result[0]:
            return result[0]
        
        return None
        
    except Exception as e:
        logger.error(f"‚ùå Error obteniendo √∫ltima exportaci√≥n: {e}", exc_info=True)
        return None


def record_export(
    export_type: str,
    records: int,
    file_path: str,
    metadata: Optional[Dict] = None
) -> None:
    """
    Inserta registro de exportaci√≥n exitosa.
    
    Args:
        export_type: Tipo de exportaci√≥n
        records: N√∫mero de registros exportados
        file_path: Ruta del archivo generado
        metadata: Metadata adicional (JSON)
    """
    try:
        postgres = get_postgres()
        session = postgres.get_session()
        
        insert_sql = """
        INSERT INTO export_metadata (
            export_type,
            last_export_date,
            records_exported,
            output_file,
            metadata
        ) VALUES (
            :export_type,
            :last_export_date,
            :records_exported,
            :output_file,
            :metadata
        )
        """
        
        session.execute(
            text(insert_sql),
            {
                'export_type': export_type,
                'last_export_date': datetime.now(),
                'records_exported': records,
                'output_file': file_path,
                'metadata': json.dumps(metadata or {})
            }
        )
        session.commit()
        session.close()
        
        logger.info(f"‚úÖ Registro de exportaci√≥n guardado: {records} registros")
        
    except Exception as e:
        logger.error(f"‚ùå Error guardando registro de exportaci√≥n: {e}", exc_info=True)
        raise


# ============ Validaciones Pre-Exportaci√≥n ============

def validate_pre_export(days: int) -> Tuple[bool, str]:
    """
    Valida condiciones antes de exportar.
    
    Args:
        days: D√≠as hacia atr√°s para exportar
    
    Returns:
        (is_valid, error_message)
    """
    try:
        # 1. Verificar que PostgreSQL tiene datos recientes
        postgres = get_postgres()
        session = postgres.get_session()
        
        cutoff_date = datetime.now() - timedelta(days=days)
        
        sql = """
        SELECT COUNT(*) as count
        FROM performance_metrics
        WHERE timestamp >= :cutoff_date
            AND component IN ('viz', 'hybrid')
            AND success = TRUE
        """
        
        result = session.execute(
            text(sql),
            {'cutoff_date': cutoff_date}
        ).fetchone()
        
        session.close()
        
        recent_data_count = result[0] if result else 0
        
        if recent_data_count < RETRAINING_EXPORT_MIN_SAMPLES:
            return False, f"Solo hay {recent_data_count} ejemplos recientes (m√≠nimo: {RETRAINING_EXPORT_MIN_SAMPLES})"
        
        # 2. Verificar espacio en disco (>100MB)
        output_dir = Path("data/retraining")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        stat = shutil.disk_usage(output_dir)
        free_mb = stat.free / (1024 * 1024)
        
        if free_mb < 100:
            return False, f"Espacio en disco insuficiente: {free_mb:.1f}MB disponibles (m√≠nimo: 100MB)"
        
        # 3. Verificar que hay datos de calidad (success rate > 0.8)
        session = postgres.get_session()
        
        sql = """
        SELECT 
            COUNT(*) as total,
            COUNT(*) FILTER (WHERE success = TRUE) as successful
        FROM performance_metrics
        WHERE timestamp >= :cutoff_date
            AND component IN ('viz', 'hybrid')
        """
        
        result = session.execute(
            text(sql),
            {'cutoff_date': cutoff_date}
        ).fetchone()
        
        session.close()
        
        total = result[0] if result else 0
        successful = result[1] if result else 0
        
        if total > 0:
            success_rate = successful / total
            if success_rate < 0.7:
                return False, f"Success rate muy bajo: {success_rate:.1%} (m√≠nimo recomendado: 70%)"
        
        return True, "Validaciones pasadas"
        
    except Exception as e:
        logger.error(f"‚ùå Error en validaciones: {e}", exc_info=True)
        return False, f"Error en validaciones: {str(e)}"


# ============ Cleanup Autom√°tico ============

def cleanup_old_exports() -> int:
    """
    Elimina exports antiguos y mantiene solo los N m√°s recientes.
    
    Returns:
        N√∫mero de archivos eliminados
    """
    try:
        output_dir = Path("data/retraining")
        
        if not output_dir.exists():
            return 0
        
        # Obtener todos los archivos de exportaci√≥n
        export_files = list(output_dir.glob("training_data_*.jsonl"))
        
        if len(export_files) <= RETRAINING_KEEP_LAST_N:
            return 0
        
        # Ordenar por fecha de modificaci√≥n (m√°s recientes primero)
        export_files.sort(key=lambda f: f.stat().st_mtime, reverse=True)
        
        # Eliminar los antiguos
        files_to_delete = export_files[RETRAINING_KEEP_LAST_N:]
        deleted_count = 0
        
        cutoff_date = datetime.now() - timedelta(days=RETRAINING_CLEANUP_DAYS)
        
        for file_path in files_to_delete:
            # Eliminar si es m√°s antiguo que RETRAINING_CLEANUP_DAYS
            file_mtime = datetime.fromtimestamp(file_path.stat().st_mtime)
            
            if file_mtime < cutoff_date:
                try:
                    file_path.unlink()
                    deleted_count += 1
                    logger.info(f"üóëÔ∏è  Eliminado archivo antiguo: {file_path.name}")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è  Error eliminando {file_path.name}: {e}")
        
        return deleted_count
        
    except Exception as e:
        logger.error(f"‚ùå Error en cleanup: {e}", exc_info=True)
        return 0


# ============ Generaci√≥n de Reporte ============

def generate_export_report(output_file: str) -> Dict:
    """
    Genera reporte detallado de la exportaci√≥n.
    
    Args:
        output_file: Ruta del archivo exportado
    
    Returns:
        Dict con estad√≠sticas del reporte
    """
    try:
        report = {
            "total_samples": 0,
            "chart_type_distribution": {},
            "avg_confidence": 0.0,
            "file_size_mb": 0.0,
            "sources": {"performance_metrics": 0, "feedback": 0}
        }
        
        # Leer archivo y analizar
        chart_type_counts = {}
        confidence_values = []
        
        with open(output_file, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    data = json.loads(line)
                    report["total_samples"] += 1
                    
                    # Extraer chart_type
                    assistant_content = data['messages'][2]['content']
                    assistant_data = json.loads(assistant_content)
                    chart_type = assistant_data.get('chart_type', 'unknown')
                    chart_type_counts[chart_type] = chart_type_counts.get(chart_type, 0) + 1
                    
                    # Extraer confidence
                    confidence = assistant_data.get('confidence', 0.0)
                    if confidence > 0:
                        confidence_values.append(confidence)
                    
                    # Detectar fuente
                    if assistant_data.get('source') == 'user_feedback':
                        report["sources"]["feedback"] += 1
                    else:
                        report["sources"]["performance_metrics"] += 1
                        
                except Exception as e:
                    logger.warning(f"Error procesando l√≠nea del reporte: {e}")
        
        report["chart_type_distribution"] = chart_type_counts
        
        if confidence_values:
            report["avg_confidence"] = sum(confidence_values) / len(confidence_values)
        
        # Tama√±o del archivo
        file_path = Path(output_file)
        if file_path.exists():
            report["file_size_mb"] = file_path.stat().st_size / (1024 * 1024)
        
        return report
        
    except Exception as e:
        logger.error(f"‚ùå Error generando reporte: {e}", exc_info=True)
        return {}


# ============ Funci√≥n Principal ============

def check_and_export(
    force: bool = False,
    dry_run: bool = False,
    days: Optional[int] = None
) -> bool:
    """
    Funci√≥n principal: verifica y ejecuta exportaci√≥n si es necesario.
    
    Args:
        force: Forzar exportaci√≥n sin verificar fecha
        dry_run: Simular sin exportar realmente
        days: D√≠as hacia atr√°s (default: usar configuraci√≥n)
    
    Returns:
        True si exportaci√≥n fue exitosa o no necesaria, False si fall√≥
    """
    start_time = time.time()
    
    try:
        logger.info("=" * 60)
        logger.info("üìä Iniciando verificaci√≥n de exportaci√≥n semanal")
        logger.info("=" * 60)
        
        # Verificar si est√° habilitado
        if not RETRAINING_EXPORT_ENABLED:
            logger.info("‚ö†Ô∏è  Exportaci√≥n autom√°tica deshabilitada (RETRAINING_EXPORT_ENABLED=false)")
            return True
        
        # Crear tabla de metadata si no existe
        create_export_metadata_table()
        
        # Determinar d√≠as a exportar
        export_days = days if days is not None else RETRAINING_EXPORT_INTERVAL_DAYS
        
        # Verificar √∫ltima exportaci√≥n (a menos que sea forzado)
        if not force:
            last_export = get_last_export_date()
            
            if last_export:
                days_since_last = (datetime.now() - last_export).days
                logger.info(f"üìÖ √öltima exportaci√≥n: {last_export.strftime('%Y-%m-%d %H:%M:%S')}")
                logger.info(f"üìÖ D√≠as desde √∫ltima exportaci√≥n: {days_since_last}")
                
                if days_since_last < RETRAINING_EXPORT_INTERVAL_DAYS:
                    logger.info(f"‚úÖ No es necesario exportar a√∫n (faltan {RETRAINING_EXPORT_INTERVAL_DAYS - days_since_last} d√≠as)")
                    return True
            else:
                logger.info("üìÖ No hay exportaciones previas registradas")
        
        # Validaciones pre-exportaci√≥n
        logger.info("üîç Ejecutando validaciones pre-exportaci√≥n...")
        is_valid, validation_message = validate_pre_export(export_days)
        
        if not is_valid:
            logger.warning(f"‚ö†Ô∏è  Validaciones fallaron: {validation_message}")
            logger.warning("‚ö†Ô∏è  Abortando exportaci√≥n")
            return False
        
        logger.info(f"‚úÖ {validation_message}")
        
        # Dry-run mode
        if dry_run:
            logger.info("üîç DRY-RUN: Simulando exportaci√≥n (no se exportar√° realmente)")
            logger.info(f"   D√≠as: {export_days}")
            logger.info(f"   M√≠nimo de muestras: {RETRAINING_EXPORT_MIN_SAMPLES}")
            return True
        
        # Ejecutar exportaci√≥n
        logger.info(f"üì§ Iniciando exportaci√≥n (√∫ltimos {export_days} d√≠as)...")
        
        output_file = export_training_data(
            days=export_days,
            min_confidence=0.8,
            output_dir="data/retraining"
        )
        
        # Generar reporte
        logger.info("üìä Generando reporte de exportaci√≥n...")
        report = generate_export_report(output_file)
        
        # Registrar exportaci√≥n
        record_export(
            export_type="retraining",
            records=report.get("total_samples", 0),
            file_path=output_file,
            metadata={
                "chart_type_distribution": report.get("chart_type_distribution", {}),
                "avg_confidence": report.get("avg_confidence", 0.0),
                "file_size_mb": report.get("file_size_mb", 0.0),
                "sources": report.get("sources", {}),
                "days_exported": export_days
            }
        )
        
        # Registrar m√©trica de exportaci√≥n
        latency_ms = int((time.time() - start_time) * 1000)
        
        try:
            track_hybrid_execution(
                query="retraining_export",
                success=True,
                latency_ms=latency_ms,
                metadata={
                    "component": "retraining_export",
                    "records_exported": report.get("total_samples", 0),
                    "file_size_mb": report.get("file_size_mb", 0.0),
                    "chart_type_distribution": report.get("chart_type_distribution", {}),
                    "avg_confidence": report.get("avg_confidence", 0.0)
                }
            )
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Error registrando m√©trica: {e}")
        
        # Logging de reporte
        logger.info("=" * 60)
        logger.info("üìä REPORTE DE EXPORTACI√ìN")
        logger.info("=" * 60)
        logger.info(f"‚úÖ Total de ejemplos exportados: {report.get('total_samples', 0)}")
        logger.info(f"üìÅ Archivo: {output_file}")
        logger.info(f"üíæ Tama√±o: {report.get('file_size_mb', 0.0):.2f} MB")
        logger.info(f"üìä Confianza promedio: {report.get('avg_confidence', 0.0):.2%}")
        logger.info(f"‚è±Ô∏è  Tiempo de exportaci√≥n: {latency_ms}ms")
        
        logger.info("\nüìä Distribuci√≥n por tipo de gr√°fica:")
        for chart_type, count in sorted(
            report.get("chart_type_distribution", {}).items(),
            key=lambda x: -x[1]
        ):
            logger.info(f"   {chart_type}: {count}")
        
        logger.info("\nüìä Fuentes de datos:")
        sources = report.get("sources", {})
        logger.info(f"   Performance Metrics: {sources.get('performance_metrics', 0)}")
        logger.info(f"   User Feedback: {sources.get('feedback', 0)}")
        
        # Cleanup
        logger.info("\nüßπ Ejecutando cleanup de archivos antiguos...")
        deleted_count = cleanup_old_exports()
        if deleted_count > 0:
            logger.info(f"üóëÔ∏è  Eliminados {deleted_count} archivos antiguos")
        else:
            logger.info("‚úÖ No hay archivos antiguos para eliminar")
        
        logger.info("=" * 60)
        logger.info("‚úÖ Exportaci√≥n completada exitosamente")
        logger.info("=" * 60)
        
        return True
        
    except Exception as e:
        logger.error("=" * 60)
        logger.error("‚ùå ERROR EN EXPORTACI√ìN")
        logger.error("=" * 60)
        logger.error(f"‚ùå Error: {e}", exc_info=True)
        
        # Registrar m√©trica de error
        try:
            latency_ms = int((time.time() - start_time) * 1000)
            track_hybrid_execution(
                query="retraining_export",
                success=False,
                latency_ms=latency_ms,
                error_message=str(e)
            )
        except:
            pass
        
        return False


# ============ Main ============

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Script de monitoreo semanal para exportaci√≥n de datos de reentrenamiento"
    )
    parser.add_argument(
        "--force",
        action="store_true",
        help="Forzar exportaci√≥n sin verificar fecha"
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Simular exportaci√≥n sin ejecutar realmente"
    )
    parser.add_argument(
        "--days",
        type=int,
        default=None,
        help=f"D√≠as hacia atr√°s para exportar (default: {RETRAINING_EXPORT_INTERVAL_DAYS})"
    )
    
    args = parser.parse_args()
    
    try:
        success = check_and_export(
            force=args.force,
            dry_run=args.dry_run,
            days=args.days
        )
        
        sys.exit(0 if success else 1)
        
    except KeyboardInterrupt:
        logger.warning("‚ö†Ô∏è  Exportaci√≥n cancelada por el usuario")
        sys.exit(130)
    except Exception as e:
        logger.error(f"‚ùå Error fatal: {e}", exc_info=True)
        sys.exit(1)

