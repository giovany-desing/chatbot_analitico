"""
Configuraci√≥n de LLMs y embeddings.
Gestiona modelos, prompts, retry logic y fallbacks.
"""

import sys
import os
from pathlib import Path

# Agregar el directorio ra√≠z del proyecto al PYTHONPATH si se ejecuta directamente
if __name__ == "__main__":
    # Obtener el directorio ra√≠z del proyecto (2 niveles arriba de este archivo)
    project_root = Path(__file__).parent.parent.parent
    if str(project_root) not in sys.path:
        sys.path.insert(0, str(project_root))

from langchain_groq import ChatGroq
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from sentence_transformers import SentenceTransformer
from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type
)
from typing import Optional, List
import logging
from functools import lru_cache

from app.config import settings

logger = logging.getLogger(__name__)


# ============ LLM Configuration ============

@lru_cache()
def get_llama_model() -> ChatGroq:
    """
    Retorna instancia de Llama 3.3 70B via Groq.
    Usa cache para reutilizar la misma instancia.
    
    Configuraci√≥n:
    - Temperature: 0.1 (preciso para SQL y an√°lisis)
    - Max tokens: 2000 (suficiente para queries complejas)
    - Timeout: 30s
    
    Returns:
        ChatGroq configurado
    """
    try:
        llm = ChatGroq(
            model=settings.LLM_MODEL,
            temperature=settings.LLM_TEMPERATURE,
            max_tokens=settings.LLM_MAX_TOKENS,
            timeout=settings.LLM_TIMEOUT,
            groq_api_key=settings.GROQ_API_KEY,
            max_retries=3
        )

        logger.info(f"‚úÖ Llama model initialized: {settings.LLM_MODEL}")
        return llm

    except Exception as e:
        logger.error(f"‚ùå Error initializing Llama model: {e}")
        raise


@lru_cache()
def get_fallback_model() -> Optional[ChatOpenAI]:
    """
    Retorna GPT-4 como fallback si Groq falla.
    Solo se inicializa si OPENAI_API_KEY est√° configurada.
    
    Returns:
        ChatOpenAI o None si no hay API key
    """
    if not settings.OPENAI_API_KEY:
        logger.warning("‚ö†Ô∏è No OPENAI_API_KEY configured, no fallback available")
        return None

    try:
        llm = ChatOpenAI(
            model="gpt-4-turbo-preview",
            temperature=settings.LLM_TEMPERATURE,
            max_tokens=settings.LLM_MAX_TOKENS,
            timeout=settings.LLM_TIMEOUT,
            openai_api_key=settings.OPENAI_API_KEY
        )

        logger.info("‚úÖ GPT-4 fallback model initialized")
        return llm

    except Exception as e:
        logger.error(f"‚ùå Error initializing fallback model: {e}")
        return None


@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=2, max=10),
    retry=retry_if_exception_type(Exception)
)
def invoke_llm_with_retry(llm, messages: list) -> str:
    """
    Invoca el LLM con retry autom√°tico.
    Si falla 3 veces, intenta con fallback.
    
    Args:
        llm: Instancia del LLM
        messages: Lista de mensajes
    
    Returns:
        Respuesta del LLM
    """
    try:
        response = llm.invoke(messages)
        return response.content

    except Exception as e:
        logger.error(f"Error invoking LLM: {e}")

        # Intentar fallback
        fallback = get_fallback_model()
        if fallback:
            logger.info("üîÑ Using GPT-4 fallback")
            response = fallback.invoke(messages)
            return response.content

        raise


# ============ Embeddings Configuration ============

@lru_cache()
def get_embedding_model() -> SentenceTransformer:
    """
    Retorna modelo de embeddings multiling√ºe.
    
    Modelo: paraphrase-multilingual-mpnet-base-v2
    - Dimensi√≥n: 768
    - Idiomas: 50+ incluyendo espa√±ol
    - Optimizado para similitud sem√°ntica
    
    Returns:
        SentenceTransformer
    """
    try:
        model = SentenceTransformer(
            'sentence-transformers/paraphrase-multilingual-mpnet-base-v2',
            device='cpu'  # Cambiar a 'cuda' si tienes GPU
        )

        logger.info("‚úÖ Embedding model initialized (768 dim)")
        return model

    except Exception as e:
        logger.error(f"‚ùå Error initializing embedding model: {e}")
        raise


def generate_embeddings(texts: List[str]) -> List[List[float]]:
    """
    Genera embeddings para una lista de textos.
    
    Args:
        texts: Lista de strings
    
    Returns:
        Lista de embeddings (cada uno es una lista de 768 floats)
    """
    model = get_embedding_model()
    embeddings = model.encode(texts, convert_to_numpy=True)
    return embeddings.tolist()


def generate_single_embedding(text: str) -> List[float]:
    """
    Genera embedding para un solo texto.
    
    Args:
        text: String a embedder
    
    Returns:
        Lista de 768 floats
    """
    return generate_embeddings([text])[0]


# ============ Prompts Templates ============

def get_router_prompt() -> ChatPromptTemplate:
    """
    Prompt para clasificar la intenci√≥n del usuario.
    
    Clasifica en:
    - sql: Necesita datos de la BD
    - kpi: Necesita calcular m√©tricas
    - viz: Necesita gr√°ficas
    - general: Pregunta general sin datos
    - hybrid: Combinaci√≥n de las anteriores
    
    Returns:
        ChatPromptTemplate
    """
    template = """Eres un clasificador de intenciones para un chatbot anal√≠tico de ventas.

Tu tarea es clasificar la pregunta del usuario en UNA de estas categor√≠as:

1. **sql**: El usuario pide datos espec√≠ficos de la base de datos
   Ejemplos: "¬øCu√°ntas ventas hubo en enero?", "Mu√©strame los productos m√°s vendidos"

2. **kpi**: El usuario pide calcular m√©tricas o KPIs
   Ejemplos: "¬øCu√°l es el revenue total?", "Calcula el ticket promedio"

3. **viz**: El usuario pide una visualizaci√≥n o gr√°fica
   Ejemplos: "Mu√©strame una gr√°fica de ventas", "Graf√≠came los productos"

4. **general**: Pregunta general sin necesidad de datos
   Ejemplos: "¬øQu√© puedes hacer?", "Hola", "Expl√≠came qu√© es un KPI"

5. **hybrid**: Necesita combinar SQL + KPIs + Visualizaci√≥n
   Ejemplos: "Dame las ventas del √∫ltimo mes y graf√≠calas", "Calcula el revenue y mu√©stralo en gr√°fica"

Pregunta del usuario: {user_query}

Responde √öNICAMENTE con una palabra: sql, kpi, viz, general, o hybrid
No des explicaciones, solo la categor√≠a."""

    return ChatPromptTemplate.from_template(template)


def get_sql_prompt() -> ChatPromptTemplate:
    """
    Prompt para generar queries SQL desde lenguaje natural.
    
    Returns:
        ChatPromptTemplate
    """
    template = """Eres un experto en SQL especializado en bases de datos de ventas.

**ESQUEMA DE LA BASE DE DATOS:**

{schema_info}

**EJEMPLOS DE QUERIES:**

{examples}

**REGLAS IMPORTANTES:**

1. Genera SOLO la query SQL, sin explicaciones
2. Usa sintaxis MySQL 8.0
3. NO uses DELETE, UPDATE, DROP, ALTER, CREATE
4. Usa ONLY SELECT queries
5. Parametriza fechas en formato 'YYYY-MM-DD'
6. Usa JOINs cuando sea necesario
7. Agrega LIMIT 100 si no especifican l√≠mite
8. Comenta queries complejas con -- comentarios

**PREGUNTA DEL USUARIO:**

{user_query}

**QUERY SQL:**"""

    return ChatPromptTemplate.from_template(template)


def get_kpi_prompt() -> ChatPromptTemplate:
    """
    Prompt para calcular KPIs desde resultados SQL.
    
    Returns:
        ChatPromptTemplate
    """
    template = """Eres un analista de datos especializado en KPIs de ventas.

**DATOS DISPONIBLES:**

{sql_results}

**KPIs COMUNES:**

- Revenue Total: SUM(total)
- Ticket Promedio: SUM(total) / COUNT(DISTINCT venta_id)
- Unidades Vendidas: SUM(cantidad)
- Productos √önicos: COUNT(DISTINCT producto_id)
- Clientes √önicos: COUNT(DISTINCT cliente_id)
- Crecimiento: ((periodo_actual - periodo_anterior) / periodo_anterior) * 100

**PREGUNTA DEL USUARIO:**

{user_query}

**INSTRUCCIONES:**

1. Calcula los KPIs solicitados
2. Explica brevemente qu√© significa cada KPI
3. Incluye el valor num√©rico formateado
4. Si hay comparaciones, calcula el % de cambio

**RESPUESTA:**"""

    return ChatPromptTemplate.from_template(template)


def get_viz_prompt() -> ChatPromptTemplate:
    """
    Prompt para decidir qu√© tipo de gr√°fica crear.
    
    Returns:
        ChatPromptTemplate
    """
    template = """Eres un experto en visualizaci√≥n de datos.

**DATOS DISPONIBLES:**

{sql_results}

**TIPOS DE GR√ÅFICAS DISPONIBLES:**

1. **bar**: Comparar categor√≠as (productos, ciudades, etc.)
2. **line**: Tendencias en el tiempo (ventas por mes, etc.)
3. **pie**: Proporciones (% de ventas por categor√≠a)
4. **scatter**: Relaciones entre variables (precio vs cantidad)
5. **histogram**: Distribuci√≥n de valores (distribuci√≥n de precios)

**PREGUNTA DEL USUARIO:**

{user_query}

**INSTRUCCIONES:**

Analiza los datos y decide:
1. Tipo de gr√°fica m√°s apropiado
2. Columna para eje X
3. Columna para eje Y
4. T√≠tulo de la gr√°fica
5. Etiquetas de ejes

Responde en formato JSON:

{{
    "chart_type": "bar|line|pie|scatter|histogram",
    "x_column": "nombre_columna",
    "y_column": "nombre_columna",
    "title": "T√≠tulo descriptivo",
    "x_label": "Etiqueta eje X",
    "y_label": "Etiqueta eje Y"
}}

**RESPUESTA JSON:**"""

    return ChatPromptTemplate.from_template(template)


def get_general_prompt() -> ChatPromptTemplate:
    """
    Prompt para respuestas generales sin acceso a datos.
    
    Returns:
        ChatPromptTemplate
    """
    template = """Eres un asistente virtual especializado en an√°lisis de ventas.

**TUS CAPACIDADES:**

- Responder preguntas sobre ventas en la base de datos
- Generar queries SQL para analizar datos
- Calcular KPIs (revenue, ticket promedio, etc.)
- Crear gr√°ficas y visualizaciones
- Explicar conceptos de an√°lisis de datos

**CONTEXTO:**

Tienes acceso a una base de datos con tablas: productos, clientes, ventas.

**PREGUNTA DEL USUARIO:**

{user_query}

**INSTRUCCIONES:**

1. Responde de forma concisa y amigable
2. Si preguntan qu√© puedes hacer, lista tus capacidades
3. Si preguntan por datos espec√≠ficos, explica que necesitas m√°s detalles
4. Mant√©n un tono profesional pero cercano

**RESPUESTA:**"""

    return ChatPromptTemplate.from_template(template)


# ============ Helper Functions ============

def test_llm_connection() -> bool:
    """
    Test b√°sico de conectividad con el LLM.
    
    Returns:
        True si funciona, False si falla
    """
    try:
        llm = get_llama_model()
        response = invoke_llm_with_retry(
            llm,
            [{"role": "user", "content": "Responde solo 'OK' si me entiendes"}]
        )

        success = "ok" in response.lower()
        if success:
            logger.info("‚úÖ LLM connection test passed")
        else:
            logger.warning(f"‚ö†Ô∏è Unexpected LLM response: {response}")

        return success

    except Exception as e:
        logger.error(f"‚ùå LLM connection test failed: {e}")
        return False


def test_embeddings() -> bool:
    """
    Test b√°sico de generaci√≥n de embeddings.
    
    Returns:
        True si funciona, False si falla
    """
    try:
        embedding = generate_single_embedding("test")

        # Verificar dimensi√≥n correcta
        if len(embedding) == 768:
            logger.info("‚úÖ Embeddings test passed (768 dim)")
            return True
        else:
            logger.error(f"‚ùå Wrong embedding dimension: {len(embedding)}")
            return False

    except Exception as e:
        logger.error(f"‚ùå Embeddings test failed: {e}")
        return False


# Para testing
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)

    print("=== Testing LLM Configuration ===\n")

    # Test 1: Inicializaci√≥n
    print("1. Initializing models...")
    llm = get_llama_model()
    embedding_model = get_embedding_model()
    print("   ‚úì Models loaded\n")

    # Test 2: LLM b√°sico
    print("2. Testing LLM:")
    response = invoke_llm_with_retry(
        llm,
        [{"role": "user", "content": "¬øCu√°l es la capital de Colombia?"}]
    )
    print(f"   Question: ¬øCu√°l es la capital de Colombia?")
    print(f"   Answer: {response}\n")

    # Test 3: Embeddings
    print("3. Testing embeddings:")
    text = "Este es un texto de prueba"
    embedding = generate_single_embedding(text)
    print(f"   Text: {text}")
    print(f"   Embedding dimension: {len(embedding)}")
    print(f"   First 5 values: {embedding[:5]}\n")

    # Test 4: Prompts
    print("4. Testing prompts:")
    router_prompt = get_router_prompt()
    print(f"   Router prompt loaded: {len(router_prompt.messages)} messages")

    sql_prompt = get_sql_prompt()
    print(f"   SQL prompt loaded: {len(sql_prompt.messages)} messages\n")

    # Test 5: Router con LLM
    print("5. Testing router classification:")
    test_queries = [
        "¬øCu√°ntas ventas hubo en enero?",
        "Calcula el revenue total",
        "Mu√©strame una gr√°fica de ventas",
        "Hola, ¬øc√≥mo est√°s?"
    ]

    for query in test_queries:
        prompt = router_prompt.format(user_query=query)
        intent = invoke_llm_with_retry(llm, [{"role": "user", "content": prompt}])
        print(f"   Query: {query}")
        print(f"   Intent: {intent.strip()}\n")

    # Test 6: Connection tests
    print("6. Running connection tests:")
    print(f"   LLM: {'‚úì' if test_llm_connection() else '‚úó'}")
    print(f"   Embeddings: {'‚úì' if test_embeddings() else '‚úó'}")
