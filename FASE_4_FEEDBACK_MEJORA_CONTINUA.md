# Fase 4: Sistema de Feedback y Mejora Continua

> **üìå ACTUALIZADO:** La funci√≥n `export_for_retraining()` genera datos en formato **chat** (compatible con `training_data_complete.jsonl`). El formato exportado incluye los 3 roles: system, user, assistant.

## üéØ Objetivo

Implementar un sistema de feedback que permita:
- Capturar valoraciones de usuarios sobre respuestas del chatbot
- Almacenar interacciones problem√°ticas para reentrenamiento
- Generar m√©tricas de rendimiento en tiempo real
- Crear pipeline autom√°tico de mejora continua
- Dashboard de monitoreo y analytics

## ‚úÖ Prerrequisitos

- [x] Fase 1, 2 y 3 completadas
- [x] Docker Compose funcionando
- [x] PostgreSQL con pgvector activo
- [x] Acceso a la base de datos MySQL (lecturas/escrituras)

## üìã Cambios a Implementar

### 1. Crear Tabla de Feedback en PostgreSQL

**Archivo nuevo:** `migrations/001_create_feedback_table.sql`

```sql
-- Tabla para almacenar feedback de usuarios
CREATE TABLE IF NOT EXISTS user_feedback (
    id SERIAL PRIMARY KEY,
    session_id VARCHAR(255) NOT NULL,
    user_query TEXT NOT NULL,
    sql_generated TEXT,
    chart_type VARCHAR(50),
    chart_config JSONB,
    user_rating INTEGER CHECK (user_rating BETWEEN 1 AND 5),
    feedback_text TEXT,
    error_occurred BOOLEAN DEFAULT FALSE,
    error_message TEXT,
    response_time_ms INTEGER,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,

    -- √çndices para b√∫squedas r√°pidas
    CONSTRAINT valid_rating CHECK (user_rating IS NULL OR user_rating BETWEEN 1 AND 5)
);

CREATE INDEX idx_feedback_rating ON user_feedback(user_rating);
CREATE INDEX idx_feedback_session ON user_feedback(session_id);
CREATE INDEX idx_feedback_created ON user_feedback(created_at DESC);
CREATE INDEX idx_feedback_errors ON user_feedback(error_occurred) WHERE error_occurred = TRUE;

-- Tabla para m√©tricas agregadas (cache de analytics)
CREATE TABLE IF NOT EXISTS analytics_metrics (
    id SERIAL PRIMARY KEY,
    metric_name VARCHAR(100) NOT NULL,
    metric_value NUMERIC,
    metadata JSONB,
    calculated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_metrics_name ON analytics_metrics(metric_name);
CREATE INDEX idx_metrics_date ON analytics_metrics(calculated_at DESC);
```

**Ejecutar migraci√≥n:**

```bash
# Desde el directorio ra√≠z del proyecto
docker-compose exec postgres psql -U postgres -d vectordb -f /migrations/001_create_feedback_table.sql
```

### 2. Crear Sistema de Feedback

**Archivo nuevo:** `app/feedback/feedback_service.py`

```python
from typing import Optional, Dict, List
from datetime import datetime, timedelta
import psycopg2
from psycopg2.extras import RealDictCursor
import json
from ..config import settings

class FeedbackService:
    """Servicio para gestionar feedback de usuarios y m√©tricas"""

    def __init__(self):
        self.conn_params = {
            'host': settings.POSTGRES_HOST,
            'port': settings.POSTGRES_PORT,
            'user': settings.POSTGRES_USER,
            'password': settings.POSTGRES_PASSWORD,
            'database': settings.POSTGRES_DB
        }

    def _get_connection(self):
        """Obtiene conexi√≥n a PostgreSQL"""
        return psycopg2.connect(**self.conn_params)

    def save_interaction(
        self,
        session_id: str,
        user_query: str,
        sql_generated: Optional[str] = None,
        chart_type: Optional[str] = None,
        chart_config: Optional[Dict] = None,
        response_time_ms: Optional[int] = None,
        error_occurred: bool = False,
        error_message: Optional[str] = None
    ) -> int:
        """
        Guarda una interacci√≥n del usuario
        Returns: ID del registro creado
        """
        with self._get_connection() as conn:
            with conn.cursor() as cur:
                cur.execute("""
                    INSERT INTO user_feedback (
                        session_id, user_query, sql_generated, chart_type,
                        chart_config, response_time_ms, error_occurred, error_message
                    ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
                    RETURNING id
                """, (
                    session_id, user_query, sql_generated, chart_type,
                    json.dumps(chart_config) if chart_config else None,
                    response_time_ms, error_occurred, error_message
                ))
                feedback_id = cur.fetchone()[0]
                conn.commit()
                return feedback_id

    def update_rating(
        self,
        feedback_id: int,
        rating: int,
        feedback_text: Optional[str] = None
    ) -> bool:
        """
        Actualiza la valoraci√≥n de una interacci√≥n
        Returns: True si se actualiz√≥ correctamente
        """
        if rating < 1 or rating > 5:
            raise ValueError("Rating debe estar entre 1 y 5")

        with self._get_connection() as conn:
            with conn.cursor() as cur:
                cur.execute("""
                    UPDATE user_feedback
                    SET user_rating = %s, feedback_text = %s
                    WHERE id = %s
                """, (rating, feedback_text, feedback_id))
                conn.commit()
                return cur.rowcount > 0

    def get_low_rated_queries(
        self,
        min_rating: int = 2,
        limit: int = 100
    ) -> List[Dict]:
        """
        Obtiene queries con baja valoraci√≥n para reentrenamiento
        """
        with self._get_connection() as conn:
            with conn.cursor(cursor_factory=RealDictCursor) as cur:
                cur.execute("""
                    SELECT
                        id, user_query, sql_generated, chart_type,
                        user_rating, feedback_text, created_at
                    FROM user_feedback
                    WHERE user_rating <= %s
                    ORDER BY created_at DESC
                    LIMIT %s
                """, (min_rating, limit))
                return [dict(row) for row in cur.fetchall()]

    def get_metrics(self, days: int = 7) -> Dict:
        """
        Calcula m√©tricas de rendimiento de los √∫ltimos N d√≠as
        """
        cutoff_date = datetime.now() - timedelta(days=days)

        with self._get_connection() as conn:
            with conn.cursor(cursor_factory=RealDictCursor) as cur:
                # M√©tricas generales
                cur.execute("""
                    SELECT
                        COUNT(*) as total_interactions,
                        COUNT(user_rating) as rated_interactions,
                        ROUND(AVG(user_rating), 2) as avg_rating,
                        COUNT(CASE WHEN error_occurred THEN 1 END) as errors,
                        ROUND(AVG(response_time_ms), 0) as avg_response_time_ms,
                        ROUND(PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY response_time_ms), 0) as p95_response_time_ms
                    FROM user_feedback
                    WHERE created_at >= %s
                """, (cutoff_date,))
                general_metrics = dict(cur.fetchone())

                # Distribuci√≥n de ratings
                cur.execute("""
                    SELECT
                        user_rating,
                        COUNT(*) as count
                    FROM user_feedback
                    WHERE created_at >= %s AND user_rating IS NOT NULL
                    GROUP BY user_rating
                    ORDER BY user_rating
                """, (cutoff_date,))
                rating_distribution = {row['user_rating']: row['count'] for row in cur.fetchall()}

                # Charts m√°s usados
                cur.execute("""
                    SELECT
                        chart_type,
                        COUNT(*) as count,
                        ROUND(AVG(user_rating), 2) as avg_rating
                    FROM user_feedback
                    WHERE created_at >= %s AND chart_type IS NOT NULL
                    GROUP BY chart_type
                    ORDER BY count DESC
                    LIMIT 10
                """, (cutoff_date,))
                top_charts = [dict(row) for row in cur.fetchall()]

                # Errores m√°s comunes
                cur.execute("""
                    SELECT
                        error_message,
                        COUNT(*) as count
                    FROM user_feedback
                    WHERE created_at >= %s AND error_occurred = TRUE
                    GROUP BY error_message
                    ORDER BY count DESC
                    LIMIT 10
                """, (cutoff_date,))
                top_errors = [dict(row) for row in cur.fetchall()]

                return {
                    'period_days': days,
                    'general': general_metrics,
                    'rating_distribution': rating_distribution,
                    'top_charts': top_charts,
                    'top_errors': top_errors
                }

    def export_for_retraining(
        self,
        output_file: str = 'retraining_data.jsonl',
        max_rating: int = 3
    ) -> int:
        """
        Exporta datos de baja calidad para reentrenamiento
        Formato compatible con training_data_complete.jsonl
        Returns: N√∫mero de ejemplos exportados
        """
        queries = self.get_low_rated_queries(min_rating=max_rating, limit=1000)

        count = 0
        with open(output_file, 'w', encoding='utf-8') as f:
            for query in queries:
                # Formato id√©ntico al dataset de entrenamiento original
                example = {
                    'messages': [
                        {
                            'role': 'system',
                            'content': 'Eres un experto en visualizaci√≥n de datos para an√°lisis de ventas textiles. Debes elegir el mejor tipo de gr√°fico bas√°ndote en la query del usuario y los datos SQL disponibles.'
                        },
                        {
                            'role': 'user',
                            'content': f"Query: {query['user_query']}\nSQL: {query['sql_generated']}\nColumnas: {['producto', 'total']}\nFilas: 10\nData preview: []"
                        },
                        {
                            'role': 'assistant',
                            'content': json.dumps({
                                'chart_type': query['chart_type'],
                                'reasoning': query['feedback_text'] or 'Necesita mejora seg√∫n feedback de usuario',
                                'confidence': 0.70,  # Baja confianza por rating bajo
                                'user_rating': query['user_rating'],
                                'needs_review': True
                            }, ensure_ascii=False)
                        }
                    ]
                }
                f.write(json.dumps(example, ensure_ascii=False) + '\n')
                count += 1

        return count

# Singleton instance
feedback_service = FeedbackService()
```

### 3. Integrar Feedback en el Workflow

**Modificar:** `app/agents/nodes.py`

Agregar al final del archivo:

```python
from ..feedback.feedback_service import feedback_service
import time

def track_interaction_node(state: State) -> State:
    """
    Nodo para rastrear la interacci√≥n y guardar m√©tricas
    Se ejecuta al final del workflow
    """
    start_time = time.time()

    try:
        # Calcular tiempo de respuesta
        response_time_ms = int((time.time() - state.get('start_time', start_time)) * 1000)

        # Guardar interacci√≥n
        feedback_id = feedback_service.save_interaction(
            session_id=state.get('session_id', 'unknown'),
            user_query=state['user_query'],
            sql_generated=state.get('sql_query'),
            chart_type=state.get('chart_config', {}).get('type'),
            chart_config=state.get('chart_config'),
            response_time_ms=response_time_ms,
            error_occurred=bool(state.get('error')),
            error_message=state.get('error')
        )

        # Agregar feedback_id al state para el frontend
        state['feedback_id'] = feedback_id

    except Exception as e:
        logger.error(f"Error guardando feedback: {e}")
        # No fallar el workflow por error de tracking

    return state
```

**Modificar:** `app/agents/graph.py`

Agregar el nodo de tracking:

```python
from .nodes import track_interaction_node

# En la funci√≥n create_graph(), agregar despu√©s de todos los nodos:

# Nodo de tracking (al final)
workflow.add_node("track", track_interaction_node)

# Conectar todos los nodos finales al tracking
workflow.add_edge("sql", "track")
workflow.add_edge("hybrid", "track")
workflow.add_edge("error", "track")

# Track es el nodo final
workflow.set_finish_point("track")
```

### 4. Agregar Endpoints de Feedback a la API

**Modificar:** `app/main.py`

Agregar despu√©s de los imports existentes:

```python
from .feedback.feedback_service import feedback_service
from pydantic import BaseModel, Field

class FeedbackRequest(BaseModel):
    feedback_id: int = Field(..., description="ID de la interacci√≥n")
    rating: int = Field(..., ge=1, le=5, description="Rating de 1 a 5")
    feedback_text: Optional[str] = Field(None, description="Comentario opcional")

class MetricsResponse(BaseModel):
    period_days: int
    general: Dict
    rating_distribution: Dict[int, int]
    top_charts: List[Dict]
    top_errors: List[Dict]
```

Agregar los endpoints al final del archivo antes de `if __name__ == "__main__"`:

```python
@app.post("/feedback", tags=["Feedback"])
async def submit_feedback(feedback: FeedbackRequest):
    """
    Enviar valoraci√≥n de una interacci√≥n
    """
    try:
        success = feedback_service.update_rating(
            feedback_id=feedback.feedback_id,
            rating=feedback.rating,
            feedback_text=feedback.feedback_text
        )
        if success:
            return {"status": "success", "message": "Feedback guardado correctamente"}
        else:
            raise HTTPException(status_code=404, detail="Interacci√≥n no encontrada")
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error guardando feedback: {str(e)}")

@app.get("/metrics", response_model=MetricsResponse, tags=["Analytics"])
async def get_metrics(days: int = 7):
    """
    Obtener m√©tricas de rendimiento de los √∫ltimos N d√≠as
    """
    try:
        metrics = feedback_service.get_metrics(days=days)
        return metrics
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error obteniendo m√©tricas: {str(e)}")

@app.get("/analytics/low-rated", tags=["Analytics"])
async def get_low_rated_queries(min_rating: int = 2, limit: int = 50):
    """
    Obtener queries con baja valoraci√≥n para an√°lisis
    """
    try:
        queries = feedback_service.get_low_rated_queries(min_rating=min_rating, limit=limit)
        return {"count": len(queries), "queries": queries}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error obteniendo queries: {str(e)}")

@app.post("/analytics/export-retraining", tags=["Analytics"])
async def export_retraining_data(max_rating: int = 3):
    """
    Exportar datos para reentrenamiento del modelo
    """
    try:
        count = feedback_service.export_for_retraining(max_rating=max_rating)
        return {
            "status": "success",
            "examples_exported": count,
            "file": "retraining_data.jsonl"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error exportando datos: {str(e)}")
```

### 5. Actualizar Frontend con Sistema de Rating

**Modificar:** `front_app.py`

Agregar despu√©s de los imports:

```python
import uuid

# Generar session_id persistente
if 'session_id' not in st.session_state:
    st.session_state.session_id = str(uuid.uuid4())
```

Agregar funci√≥n de rating despu√©s de mostrar cada respuesta (buscar donde se muestra `response['chart']` y agregar despu√©s):

```python
# Sistema de rating (agregar despu√©s de mostrar la respuesta)
if 'feedback_id' in response:
    st.markdown("---")
    st.markdown("**¬øQu√© te pareci√≥ esta respuesta?**")

    col1, col2, col3, col4, col5, col6 = st.columns([1, 1, 1, 1, 1, 3])

    rating = None
    with col1:
        if st.button("‚≠ê", key=f"rate1_{response['feedback_id']}"):
            rating = 1
    with col2:
        if st.button("‚≠ê‚≠ê", key=f"rate2_{response['feedback_id']}"):
            rating = 2
    with col3:
        if st.button("‚≠ê‚≠ê‚≠ê", key=f"rate3_{response['feedback_id']}"):
            rating = 3
    with col4:
        if st.button("‚≠ê‚≠ê‚≠ê‚≠ê", key=f"rate4_{response['feedback_id']}"):
            rating = 4
    with col5:
        if st.button("‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê", key=f"rate5_{response['feedback_id']}"):
            rating = 5

    if rating:
        # Opcional: pedir comentario para ratings bajos
        feedback_text = None
        if rating <= 3:
            feedback_text = st.text_input(
                "¬øQu√© podemos mejorar?",
                key=f"feedback_text_{response['feedback_id']}"
            )

        # Enviar feedback
        try:
            feedback_response = requests.post(
                f"{API_URL}/feedback",
                json={
                    "feedback_id": response['feedback_id'],
                    "rating": rating,
                    "feedback_text": feedback_text
                },
                timeout=5
            )
            if feedback_response.status_code == 200:
                st.success(f"¬°Gracias por tu valoraci√≥n de {rating} estrellas!")
        except Exception as e:
            st.error(f"Error enviando feedback: {e}")
```

Agregar p√°gina de m√©tricas en el sidebar:

```python
# En el sidebar, agregar despu√©s de los ejemplos:
st.sidebar.markdown("---")
st.sidebar.markdown("### üìä Analytics")

if st.sidebar.button("Ver M√©tricas"):
    try:
        metrics_response = requests.get(f"{API_URL}/metrics?days=7", timeout=10)
        if metrics_response.status_code == 200:
            metrics = metrics_response.json()

            st.markdown("## üìà M√©tricas de los √öltimos 7 D√≠as")

            # M√©tricas generales
            gen = metrics['general']
            col1, col2, col3 = st.columns(3)
            col1.metric("Total Interacciones", gen['total_interactions'])
            col2.metric("Rating Promedio", f"{gen['avg_rating']}/5.0")
            col3.metric("Tiempo Respuesta (avg)", f"{gen['avg_response_time_ms']}ms")

            # Distribuci√≥n de ratings
            if metrics['rating_distribution']:
                st.markdown("### Distribuci√≥n de Ratings")
                rating_df = pd.DataFrame([
                    {'Rating': f"{k}‚≠ê", 'Cantidad': v}
                    for k, v in metrics['rating_distribution'].items()
                ])
                st.bar_chart(rating_df.set_index('Rating'))

            # Charts m√°s usados
            if metrics['top_charts']:
                st.markdown("### Gr√°ficos M√°s Usados")
                charts_df = pd.DataFrame(metrics['top_charts'])
                st.dataframe(charts_df)

            # Errores comunes
            if metrics['top_errors']:
                st.markdown("### Errores M√°s Comunes")
                errors_df = pd.DataFrame(metrics['top_errors'])
                st.dataframe(errors_df)
    except Exception as e:
        st.error(f"Error obteniendo m√©tricas: {e}")
```

### 6. Actualizar docker-compose.yml

**Modificar:** `docker-compose.yml`

Agregar volumen para migraciones en el servicio `postgres`. Busca la secci√≥n `volumes:` del servicio `postgres` y agrega la l√≠nea para montar el directorio de migraciones:

**Antes:**
```yaml
  postgres:
    image: ankane/pgvector:latest
    container_name: chatbot_postgres
    ports:
      - "5432:5432"
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./scripts/init_postgres.sql:/docker-entrypoint-initdb.d/init.sql
```

**Despu√©s:**
```yaml
  postgres:
    image: ankane/pgvector:latest
    container_name: chatbot_postgres
    ports:
      - "5432:5432"
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./scripts/init_postgres.sql:/docker-entrypoint-initdb.d/init.sql
      - ./migrations:/migrations  # ‚Üê Agregar esta l√≠nea para montar el directorio de migraciones
```

**Explicaci√≥n:**
- `./migrations:/migrations` monta el directorio local `./migrations` en `/migrations` dentro del contenedor
- Esto permite ejecutar migraciones SQL directamente desde el contenedor usando la ruta `/migrations/`
- Despu√©s de agregar esta l√≠nea, reinicia el contenedor: `docker-compose restart postgres`

### 7. Crear Script de Reentrenamiento Autom√°tico

**Archivo nuevo:** `scripts/auto_retrain.py`

```python
#!/usr/bin/env python3
"""
Script para reentrenamiento autom√°tico basado en feedback
Ejecutar semanalmente via cron o scheduler
"""
import sys
import os
from pathlib import Path

# Agregar path del proyecto
sys.path.append(str(Path(__file__).parent.parent))

from app.feedback.feedback_service import feedback_service
from datetime import datetime

def main():
    print("üîÑ Iniciando proceso de reentrenamiento autom√°tico")
    print(f"üìÖ Fecha: {datetime.now().isoformat()}")

    # 1. Obtener m√©tricas
    print("\nüìä Obteniendo m√©tricas...")
    metrics = feedback_service.get_metrics(days=7)

    avg_rating = metrics['general']['avg_rating']
    total_interactions = metrics['general']['total_interactions']

    print(f"   Total interacciones: {total_interactions}")
    print(f"   Rating promedio: {avg_rating}/5.0")

    # 2. Decidir si reentrenar
    THRESHOLD_RATING = 3.5
    THRESHOLD_INTERACTIONS = 100

    if avg_rating < THRESHOLD_RATING and total_interactions >= THRESHOLD_INTERACTIONS:
        print(f"\n‚ö†Ô∏è  Rating bajo ({avg_rating}) - Iniciando reentrenamiento...")

        # 3. Exportar datos
        output_file = f"retraining_data_{datetime.now().strftime('%Y%m%d')}.jsonl"
        count = feedback_service.export_for_retraining(
            output_file=output_file,
            max_rating=3
        )

        print(f"‚úÖ Exportados {count} ejemplos a {output_file}")
        print(f"üìù Siguiente paso: Combinar con dataset original y subir a Google Colab")
        print(f"üí° Comando: cat training_data_complete.jsonl {output_file} > training_v2.jsonl")
        print(f"üìñ Ver FASE_1_FINE_TUNING_ACTUALIZADO.md para reentrenamiento")

        return count
    else:
        print(f"\n‚úÖ Sistema funcionando bien (rating: {avg_rating})")
        print("   No es necesario reentrenar")
        return 0

if __name__ == "__main__":
    exported = main()
    sys.exit(0 if exported >= 0 else 1)
```

Hacer ejecutable:

```bash
chmod +x scripts/auto_retrain.py
```

## üì¶ Librer√≠as Adicionales

Agregar a `requirements.txt`:

```txt
# Ya existentes (verificar que est√©n)
psycopg2-binary>=2.9.9
```

No se requieren librer√≠as adicionales, todo usa dependencias existentes.

## üß™ Plan de Pruebas (Reproducible)

### Prueba 1: Verificar Migraci√≥n de Base de Datos

**Objetivo:** Confirmar que las tablas de feedback se crearon correctamente

```bash
# 1. Crear directorio de migraciones
mkdir -p migrations

# 2. Copiar el SQL de la secci√≥n 1 a migrations/001_create_feedback_table.sql

# 3. Ejecutar migraci√≥n
docker-compose exec postgres psql -U postgres -d vectordb -f /migrations/001_create_feedback_table.sql

# 4. Verificar tablas creadas
docker-compose exec postgres psql -U postgres -d vectordb -c "\dt"
```

**Output esperado:**

```
              List of relations
 Schema |       Name        | Type  |  Owner
--------+-------------------+-------+----------
 public | analytics_metrics | table | postgres
 public | user_feedback     | table | postgres
(2 rows)
```

### Prueba 2: Test del FeedbackService

**Crear archivo:** `tests/test_feedback_service.py`

```python
import pytest
from app.feedback.feedback_service import feedback_service
import time

def test_save_and_update_interaction():
    """Test completo del ciclo de feedback"""

    # 1. Guardar interacci√≥n
    feedback_id = feedback_service.save_interaction(
        session_id="test_session_123",
        user_query="¬øCu√°ntas ventas hay?",
        sql_generated="SELECT COUNT(*) FROM ordenes",
        chart_type="bar",
        chart_config={"type": "bar", "title": "Test"},
        response_time_ms=250,
        error_occurred=False
    )

    assert feedback_id > 0
    print(f"‚úÖ Interacci√≥n guardada con ID: {feedback_id}")

    # 2. Actualizar con rating
    success = feedback_service.update_rating(
        feedback_id=feedback_id,
        rating=5,
        feedback_text="Excelente respuesta"
    )

    assert success is True
    print("‚úÖ Rating actualizado correctamente")

    # 3. Verificar que no aparece en low-rated
    low_rated = feedback_service.get_low_rated_queries(min_rating=2, limit=10)
    assert not any(q['id'] == feedback_id for q in low_rated)
    print("‚úÖ No aparece en queries de baja valoraci√≥n")

def test_metrics():
    """Test de generaci√≥n de m√©tricas"""

    metrics = feedback_service.get_metrics(days=30)

    assert 'general' in metrics
    assert 'total_interactions' in metrics['general']
    assert 'rating_distribution' in metrics

    print("‚úÖ M√©tricas generadas correctamente:")
    print(f"   Total interacciones: {metrics['general']['total_interactions']}")
    print(f"   Rating promedio: {metrics['general']['avg_rating']}")

def test_export_retraining():
    """Test de exportaci√≥n para reentrenamiento"""

    # Primero crear algunos ejemplos con rating bajo
    for i in range(3):
        fid = feedback_service.save_interaction(
            session_id=f"test_{i}",
            user_query=f"Query de prueba {i}",
            chart_type="bar"
        )
        feedback_service.update_rating(fid, rating=2, feedback_text="Mejorable")

    # Exportar
    count = feedback_service.export_for_retraining(
        output_file='test_retraining.jsonl',
        max_rating=3
    )

    assert count >= 3
    print(f"‚úÖ Exportados {count} ejemplos para reentrenamiento")

if __name__ == "__main__":
    pytest.main([__file__, "-v"])
```

**Ejecutar:**

```bash
# Con flag -s para mostrar los prints de los tests
docker-compose exec app pytest tests/test_feedback_service.py -v -s
```

**Output esperado:**

```
============================= test session starts ==============================
platform linux -- Python 3.11.14, pytest-9.0.1, pluggy-1.6.0 -- /usr/local/bin/python
cachedir: .pytest_cache
rootdir: /app
plugins: timeout-2.4.0, asyncio-1.3.0, anyio-4.12.0, cov-7.0.0, langsmith-0.4.56
collecting ... collected 3 items

tests/test_feedback_service.py::test_save_and_update_interaction ‚úÖ Interacci√≥n guardada con ID: 1
‚úÖ Rating actualizado correctamente
‚úÖ No aparece en queries de baja valoraci√≥n
PASSED
tests/test_feedback_service.py::test_metrics ‚úÖ M√©tricas generadas correctamente:
   Total interacciones: 15
   Rating promedio: 4.2
PASSED
tests/test_feedback_service.py::test_export_retraining ‚úÖ Exportados 5 ejemplos para reentrenamiento
PASSED

============================== 3 passed in 1.23s ===============================
```

**Nota:** El flag `-s` (o `--capture=no`) es necesario para ver los mensajes de `print()` dentro de los tests. Sin este flag, pytest captura la salida y solo la muestra si hay errores.

### Prueba 3: Test de API Endpoints

```bash
# 1. Hacer una query para obtener un feedback_id
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{"query": "¬øCu√°ntas ventas hay?"}' | jq '.feedback_id'

# Supongamos que devuelve: 42

# 2. Enviar rating
curl -X POST http://localhost:8000/feedback \
  -H "Content-Type: application/json" \
  -d '{
    "feedback_id": 42,
    "rating": 5,
    "feedback_text": "Muy buena respuesta"
  }'

# 3. Ver m√©tricas
curl http://localhost:8000/metrics?days=7 | jq
```

**Output esperado del rating:**

```json
{
  "status": "success",
  "message": "Feedback guardado correctamente"
}
```

**Output esperado de m√©tricas:**

```json
{
  "period_days": 7,
  "general": {
    "total_interactions": 45,
    "rated_interactions": 23,
    "avg_rating": 4.35,
    "errors": 2,
    "avg_response_time_ms": 320,
    "p95_response_time_ms": 850
  },
  "rating_distribution": {
    "1": 1,
    "2": 2,
    "3": 5,
    "4": 8,
    "5": 7
  },
  "top_charts": [
    {"chart_type": "bar", "count": 18, "avg_rating": 4.5},
    {"chart_type": "line", "count": 12, "avg_rating": 4.2}
  ],
  "top_errors": []
}
```

### Prueba 4: Test de Frontend con Rating

**Pasos manuales:**

1. Abrir http://localhost:8501
2. Hacer una pregunta: "¬øCu√°ntas ventas hay?"
3. Esperar respuesta
4. Verificar que aparecen botones de rating (‚≠ê a ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê)
5. Click en "‚≠ê‚≠ê‚≠ê‚≠ê" (4 estrellas)
6. Verificar mensaje: "¬°Gracias por tu valoraci√≥n de 4 estrellas!"
7. Click en "Ver M√©tricas" en sidebar
8. Verificar que se muestra dashboard con m√©tricas

**Output esperado en Streamlit:**

```
üìà M√©tricas de los √öltimos 7 D√≠as

Total Interacciones: 45
Rating Promedio: 4.35/5.0
Tiempo Respuesta (avg): 320ms

[Gr√°fico de barras con distribuci√≥n de ratings]
[Tabla de gr√°ficos m√°s usados]
```

### Prueba 5: Script de Reentrenamiento Autom√°tico

```bash
# Ejecutar script
docker-compose exec app python scripts/auto_retrain.py
```

**Output esperado (caso 1: rating bajo):**

```
üîÑ Iniciando proceso de reentrenamiento autom√°tico
üìÖ Fecha: 2025-12-05T10:30:00

üìä Obteniendo m√©tricas...
   Total interacciones: 150
   Rating promedio: 3.2/5.0

‚ö†Ô∏è  Rating bajo (3.2) - Iniciando reentrenamiento...
‚úÖ Exportados 45 ejemplos a retraining_data_20251205.jsonl
üìù Siguiente paso: Subir retraining_data_20251205.jsonl a Google Colab para reentrenamiento
üìñ Ver FASE_1_FINE_TUNING.md secci√≥n 'Reentrenamiento'
```

**Output esperado (caso 2: sistema OK):**

```
üîÑ Iniciando proceso de reentrenamiento autom√°tico
üìÖ Fecha: 2025-12-05T10:30:00

üìä Obteniendo m√©tricas...
   Total interacciones: 150
   Rating promedio: 4.5/5.0

‚úÖ Sistema funcionando bien (rating: 4.5)
   No es necesario reentrenar
```

## ‚úÖ Checklist de Completitud

- [ ] Migraci√≥n SQL ejecutada y tablas creadas
- [ ] `feedback_service.py` creado y funcionando
- [ ] Nodo `track_interaction_node` agregado al workflow
- [ ] Endpoints `/feedback` y `/metrics` funcionando en API
- [ ] Frontend muestra botones de rating despu√©s de cada respuesta
- [ ] Dashboard de m√©tricas visible en sidebar
- [ ] Script `auto_retrain.py` ejecutable y funcionando
- [ ] Tests unitarios pasando (3/3 ‚úÖ)
- [ ] Tests de API devuelven status 200
- [ ] Datos se guardan correctamente en PostgreSQL
- [ ] M√©tricas se calculan sin errores

## üí∞ Costos

- **PostgreSQL storage**: Incluido en Docker local (0‚Ç¨)
- **CPU para analytics**: M√≠nimo, queries optimizadas con √≠ndices (0‚Ç¨)
- **Todo el sistema de feedback**: 100% GRATIS

## üéØ Pr√≥ximos Pasos

Una vez completada esta fase, tienes un sistema completo de mejora continua:

1. **Monitoreo continuo**: Dashboard de m√©tricas en tiempo real
2. **Feedback loop**: Usuarios valoran respuestas autom√°ticamente
3. **Detecci√≥n de problemas**: Queries con baja valoraci√≥n se exportan
4. **Reentrenamiento autom√°tico**: Script detecta cu√°ndo es necesario mejorar
5. **Ciclo completo**: Nuevos datos ‚Üí Reentrenamiento ‚Üí Mejora ‚Üí M√°s feedback

### Automatizaci√≥n Opcional (Cron)

Para ejecutar el script semanalmente:

```bash
# Editar crontab
crontab -e

# Agregar (ejecutar todos los lunes a las 2 AM)
0 2 * * 1 cd /path/to/chatbot_analitico && docker-compose exec app python scripts/auto_retrain.py >> logs/retrain.log 2>&1
```

## üéâ ¬°Proyecto Completo!

Has implementado exitosamente:

1. ‚úÖ **Fase 1**: Fine-tuning de modelo especializado
2. ‚úÖ **Fase 2**: Sistema h√≠brido de 3 capas
3. ‚úÖ **Fase 3**: Gr√°ficos profesionales de nivel enterprise
4. ‚úÖ **Fase 4**: Sistema de feedback y mejora continua

**Tu chatbot ahora tiene:**
- Inteligencia h√≠brida (reglas + IA + LLM)
- Visualizaciones profesionales
- Sistema de feedback integrado
- M√©tricas en tiempo real
- Pipeline de mejora continua
- Todo sin costo adicional

---

**¬°Disfruta de tu chatbot anal√≠tico de nivel empresarial!** üöÄüìä
